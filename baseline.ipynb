{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Task Type**: Image Regression\n\n**Loss Function**: 平均二乗誤差(MSE)","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"markdown","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd","metadata":{}},{"cell_type":"code","source":"import glob\nimport os\nimport random\nfrom typing import Optional\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport warnings\n\ndevice_str = \"CPU\"\nif torch.cuda.is_available():\n    device_str = f\"CUDA: {torch.cuda.get_device_name(0)}\"\nelif torch.backends.mps.is_available():\n    device_str = \"MPS (Apple Silicon GPU)\"\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"Device: {device_str}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T08:15:43.502773Z","iopub.execute_input":"2025-12-01T08:15:43.503143Z","iopub.status.idle":"2025-12-01T08:15:43.512274Z","shell.execute_reply.started":"2025-12-01T08:15:43.503111Z","shell.execute_reply":"2025-12-01T08:15:43.511583Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data","metadata":{}},{"cell_type":"code","source":"# Kaggle 上で動いているかどうかを判定\nON_KAGGLE = (\"KAGGLE_KERNEL_RUN_TYPE\" in os.environ) or Path(\"/kaggle/input\").exists()\n\nif ON_KAGGLE:\n    PATH_DATA = \"/kaggle/input/csiro-biomass\"\nelse:\n    PATH_DATA = \"data\"\n\nPATH_TRAIN_CSV = os.path.join(PATH_DATA, 'train.csv')\nPATH_TRAIN_IMG = os.path.join(PATH_DATA, 'train')\nPATH_TEST_IMG = os.path.join(PATH_DATA, 'test')\n\ndf = pd.read_csv(PATH_TRAIN_CSV)\nprint(f'Dataset size: {df.shape}')\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T08:15:43.513601Z","iopub.execute_input":"2025-12-01T08:15:43.513852Z","iopub.status.idle":"2025-12-01T08:15:43.547852Z","shell.execute_reply.started":"2025-12-01T08:15:43.513832Z","shell.execute_reply":"2025-12-01T08:15:43.547209Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TARGET_COLS = [\"target\"]\nprint(f\"Target columns: {TARGET_COLS}\")\nprint(f\"Number of targets: {len(TARGET_COLS)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T08:15:43.548557Z","iopub.execute_input":"2025-12-01T08:15:43.548778Z","iopub.status.idle":"2025-12-01T08:15:43.554540Z","shell.execute_reply.started":"2025-12-01T08:15:43.548760Z","shell.execute_reply":"2025-12-01T08:15:43.553878Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dataset/DataLoader\n\nまずは画像を読み込んで、`TARGET_COLS` と回帰ターゲットとして返す Dataset を定義する。","metadata":{}},{"cell_type":"code","source":"from torchvision import transforms\n\n# 画像サイズはとりあえず 256 にする（後で変えてOK)\nIMG_SIZE = 256\n\n# 画像前処理（最低限）\ntrain_transform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    # 正規化はひとまず 0-1 のままでも良いが、\n    # ちゃんとやるなら mean/std を計算してからここに入れる\n])\n\n\nclass BiomassDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, transform=None, is_train: bool = True):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform\n        self.is_train = is_train\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def _load_image(self, image_path: str):\n        # コンペの画像は .jpg なのでこうしておく (必要なら .png に変更)\n        img_path = os.path.join(PATH_DATA, image_path)\n        img = Image.open(img_path).convert(\"RGB\")\n        if self.transform is not None:\n            img = self.transform(img)\n        return img\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image = self._load_image(row[\"image_path\"])\n        target = torch.tensor(row[\"target\"], dtype=torch.float32)   # スカラー\n        return image, target\n    \n\nclass BiomassTestDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n    \n    def _load_image(self, image_path):\n        image_path = os.path.join(PATH_DATA, image_path)\n        img = Image.open(image_path).convert(\"RGB\")\n        if self.transform is not None:\n            img = self.transform(img)\n        return img\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image = self._load_image(row[\"image_path\"])\n        sample_id = row[\"sample_id\"]\n        return image, sample_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T08:15:43.555350Z","iopub.execute_input":"2025-12-01T08:15:43.555587Z","iopub.status.idle":"2025-12-01T08:15:43.570209Z","shell.execute_reply.started":"2025-12-01T08:15:43.555568Z","shell.execute_reply":"2025-12-01T08:15:43.569428Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"簡単に train/valid に分割して DataLoader を作る","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, valid_df = train_test_split(\n    df,\n    test_size=0.2,\n    random_state=42,\n    shuffle=True\n)\n\ntrain_dataset = BiomassDataset(train_df, transform=train_transform)\nvalid_dataset = BiomassDataset(valid_df, transform=train_transform)\n\nBATCH_SIZE = 16\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\nvalid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T08:15:43.571860Z","iopub.execute_input":"2025-12-01T08:15:43.572175Z","iopub.status.idle":"2025-12-01T08:15:43.593348Z","shell.execute_reply.started":"2025-12-01T08:15:43.572154Z","shell.execute_reply":"2025-12-01T08:15:43.592595Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"デバイス","metadata":{}},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\n\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T08:15:43.594102Z","iopub.execute_input":"2025-12-01T08:15:43.594320Z","iopub.status.idle":"2025-12-01T08:15:43.613039Z","shell.execute_reply.started":"2025-12-01T08:15:43.594299Z","shell.execute_reply":"2025-12-01T08:15:43.612407Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"シンプルな CNN ベースライン(画像 -> グローバル平均プーリング -> 全結合で5ターゲット)を定義","metadata":{}},{"cell_type":"code","source":"class SimpleCNNRegressor(nn.Module):\n    def __init__(self, num_targets: int):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),    # 256 -> 128\n\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),    # 128 -> 64\n\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),    # 64 -> 32\n\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),    # 32 -> 16\n        )\n\n        # グローバル平均プーリング\n        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.regressor = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, num_targets)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.global_pool(x)\n        x = self.regressor(x)\n        return x\n    \nEPOCHS = 10\nlr = 1e-3\n\nmodel = SimpleCNNRegressor(num_targets=len(TARGET_COLS)).to(device)\nloss_function = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T08:15:43.613797Z","iopub.execute_input":"2025-12-01T08:15:43.614026Z","iopub.status.idle":"2025-12-01T08:15:43.635776Z","shell.execute_reply.started":"2025-12-01T08:15:43.614005Z","shell.execute_reply":"2025-12-01T08:15:43.635087Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 学習ループ(MSE)\n\n最低限の train/valid ループ","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(model, loader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    running_abs_error = 0.0\n    n_samples = 0\n\n    for images, targets in tqdm(loader):\n        images = images.to(device)\n        targets = targets.to(device)    # shape: (batch, )\n\n        optimizer.zero_grad()\n        outputs = model(images).squeeze(-1)  # (batch, 1) -> (batch, )\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * images.size(0)\n        running_abs_error += torch.abs(outputs - targets).sum().item()\n        n_samples += images.size(0)\n    \n    epoch_loss = running_loss / n_samples   # MSE\n    epoch_mae = running_abs_error / n_samples   # MAE\n    epoch_rmse = np.sqrt(epoch_loss)   # RMSE\n\n    return epoch_loss, epoch_mae, epoch_rmse\n\n\ndef eval_one_epoch(model, loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    running_abs_error = 0.0\n    n_samples = 0\n\n    with torch.no_grad():\n        for images, targets in tqdm(loader):\n            images = images.to(device)\n            targets = targets.to(device)\n\n            outputs = model(images).squeeze(-1)  # (batch, 1) -> (batch, )\n            loss = criterion(outputs, targets)\n\n            running_loss += loss.item() * images.size(0)\n            running_abs_error += torch.abs(outputs - targets).sum().item()\n            n_samples += images.size(0)\n    \n    epoch_loss = running_loss / n_samples   # MSE\n    epoch_mae = running_abs_error / n_samples  # MAE\n    epoch_rmse = np.sqrt(epoch_loss)   # RMSE\n\n    return epoch_loss, epoch_mae, epoch_rmse","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T08:15:43.659215Z","iopub.execute_input":"2025-12-01T08:15:43.659700Z","iopub.status.idle":"2025-12-01T08:15:43.669379Z","shell.execute_reply.started":"2025-12-01T08:15:43.659678Z","shell.execute_reply":"2025-12-01T08:15:43.668686Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    train_loss, train_mae, train_rmse = train_one_epoch(model, train_loader, loss_function, optimizer, device)\n    valid_loss, valid_mae, valid_rmse = eval_one_epoch(model, valid_loader, loss_function, device)\n\n\n    print(\n        f\"Epoch [{epoch}/{EPOCHS}] \"\n        f\"Train [MSE: {train_loss:.4f}, RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}] \"\n        f\"Valid [MSE: {valid_loss:.4f}, RMSE: {valid_rmse:.4f}, MAE: {valid_mae:.4f}]\"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T08:15:43.670776Z","iopub.execute_input":"2025-12-01T08:15:43.670964Z","iopub.status.idle":"2025-12-01T08:31:51.344632Z","shell.execute_reply.started":"2025-12-01T08:15:43.670951Z","shell.execute_reply":"2025-12-01T08:31:51.343868Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## テスト実行","metadata":{}},{"cell_type":"code","source":"PATH_TEST_CSV = os.path.join(PATH_DATA, \"test.csv\")\ntest_df = pd.read_csv(PATH_TEST_CSV)\ntest_df.head()\n\ntest_dataset = BiomassTestDataset(test_df, transform=train_transform)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n\nmodel.eval()\nall_sample_ids = []\nall_preds = []\n\nwith torch.no_grad():\n    for images, sample_ids in tqdm(test_loader):\n        images = images.to(device)\n\n        outputs = model(images).squeeze(-1)\n        preds = outputs.cpu().numpy()\n\n        all_sample_ids.extend(sample_ids)\n        all_preds.extend(preds)\n\nsubmission = pd.DataFrame({\n    \"sample_id\": all_sample_ids,\n    \"target\": all_preds,\n})\n\nprint(submission.head())\nprint(submission.shape)\n\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T08:31:51.345560Z","iopub.execute_input":"2025-12-01T08:31:51.345842Z","iopub.status.idle":"2025-12-01T08:31:51.683453Z","shell.execute_reply.started":"2025-12-01T08:31:51.345817Z","shell.execute_reply":"2025-12-01T08:31:51.682671Z"}},"outputs":[],"execution_count":null}]}