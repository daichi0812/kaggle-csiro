{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"c556cbd5","cell_type":"markdown","source":"**Task Type**: Image Regression\n\n**Loss Function**: 平均二乗誤差(MSE)","metadata":{}},{"id":"99999b08","cell_type":"markdown","source":"## Imports","metadata":{}},{"id":"34db6986","cell_type":"markdown","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd","metadata":{}},{"id":"687e3d1d","cell_type":"code","source":"# この変更いける？\nimport glob\nimport os\nimport random\nfrom typing import Optional\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport warnings\n\ndevice_str = \"CPU\"\nif torch.cuda.is_available():\n    device_str = f\"CUDA: {torch.cuda.get_device_name(0)}\"\nelif torch.backends.mps.is_available():\n    device_str = \"MPS (Apple Silicon GPU)\"\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"Device: {device_str}\")","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["PyTorch: 2.8.0\n","Device: MPS (Apple Silicon GPU)\n"]}],"execution_count":null},{"id":"4c3a5a84","cell_type":"markdown","source":"## Data","metadata":{}},{"id":"de3744b9","cell_type":"code","source":"# Kaggle 上で動いているかどうかを判定\nON_KAGGLE = (\"KAGGLE_KERNEL_RUN_TYPE\" in os.environ) or Path(\"/kaggle/input\").exists()\n\nif ON_KAGGLE:\n    PATH_DATA = \"/kaggle/input/csiro-biomass\"\nelse:\n    PATH_DATA = \"data\"\n\nPATH_TRAIN_CSV = os.path.join(PATH_DATA, 'train.csv')\nPATH_TRAIN_IMG = os.path.join(PATH_DATA, 'train')\nPATH_TEST_IMG = os.path.join(PATH_DATA, 'test')\n\ndf = pd.read_csv(PATH_TRAIN_CSV)\nprint(f'Dataset size: {df.shape}')\ndf.head()","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset size: (1785, 9)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sample_id</th>\n","      <th>image_path</th>\n","      <th>Sampling_Date</th>\n","      <th>State</th>\n","      <th>Species</th>\n","      <th>Pre_GSHH_NDVI</th>\n","      <th>Height_Ave_cm</th>\n","      <th>target_name</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ID1011485656__Dry_Clover_g</td>\n","      <td>train/ID1011485656.jpg</td>\n","      <td>2015/9/4</td>\n","      <td>Tas</td>\n","      <td>Ryegrass_Clover</td>\n","      <td>0.62</td>\n","      <td>4.6667</td>\n","      <td>Dry_Clover_g</td>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ID1011485656__Dry_Dead_g</td>\n","      <td>train/ID1011485656.jpg</td>\n","      <td>2015/9/4</td>\n","      <td>Tas</td>\n","      <td>Ryegrass_Clover</td>\n","      <td>0.62</td>\n","      <td>4.6667</td>\n","      <td>Dry_Dead_g</td>\n","      <td>31.9984</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ID1011485656__Dry_Green_g</td>\n","      <td>train/ID1011485656.jpg</td>\n","      <td>2015/9/4</td>\n","      <td>Tas</td>\n","      <td>Ryegrass_Clover</td>\n","      <td>0.62</td>\n","      <td>4.6667</td>\n","      <td>Dry_Green_g</td>\n","      <td>16.2751</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ID1011485656__Dry_Total_g</td>\n","      <td>train/ID1011485656.jpg</td>\n","      <td>2015/9/4</td>\n","      <td>Tas</td>\n","      <td>Ryegrass_Clover</td>\n","      <td>0.62</td>\n","      <td>4.6667</td>\n","      <td>Dry_Total_g</td>\n","      <td>48.2735</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ID1011485656__GDM_g</td>\n","      <td>train/ID1011485656.jpg</td>\n","      <td>2015/9/4</td>\n","      <td>Tas</td>\n","      <td>Ryegrass_Clover</td>\n","      <td>0.62</td>\n","      <td>4.6667</td>\n","      <td>GDM_g</td>\n","      <td>16.2750</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                    sample_id              image_path Sampling_Date State  \\\n","0  ID1011485656__Dry_Clover_g  train/ID1011485656.jpg      2015/9/4   Tas   \n","1    ID1011485656__Dry_Dead_g  train/ID1011485656.jpg      2015/9/4   Tas   \n","2   ID1011485656__Dry_Green_g  train/ID1011485656.jpg      2015/9/4   Tas   \n","3   ID1011485656__Dry_Total_g  train/ID1011485656.jpg      2015/9/4   Tas   \n","4         ID1011485656__GDM_g  train/ID1011485656.jpg      2015/9/4   Tas   \n","\n","           Species  Pre_GSHH_NDVI  Height_Ave_cm   target_name   target  \n","0  Ryegrass_Clover           0.62         4.6667  Dry_Clover_g   0.0000  \n","1  Ryegrass_Clover           0.62         4.6667    Dry_Dead_g  31.9984  \n","2  Ryegrass_Clover           0.62         4.6667   Dry_Green_g  16.2751  \n","3  Ryegrass_Clover           0.62         4.6667   Dry_Total_g  48.2735  \n","4  Ryegrass_Clover           0.62         4.6667         GDM_g  16.2750  "]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"execution_count":38},{"id":"2c53f394","cell_type":"code","source":"TARGET_COLS = [\"target\"]\nprint(f\"Target columns: {TARGET_COLS}\")\nprint(f\"Number of targets: {len(TARGET_COLS)}\")","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Target columns: ['target']\n","Number of targets: 1\n"]}],"execution_count":39},{"id":"401c13a7","cell_type":"markdown","source":"### Dataset/DataLoader\n\nまずは画像を読み込んで、`TARGET_COLS` と回帰ターゲットとして返す Dataset を定義する。","metadata":{}},{"id":"2172da7a","cell_type":"code","source":"from torchvision import transforms\n\n# 画像サイズはとりあえず 256 にする（後で変えてOK)\nIMG_SIZE = 256\n\n# 画像前処理（最低限）\ntrain_transform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    # 正規化はひとまず 0-1 のままでも良いが、\n    # ちゃんとやるなら mean/std を計算してからここに入れる\n])\n\n\nclass BiomassDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, transform=None, is_train: bool = True):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform\n        self.is_train = is_train\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def _load_image(self, image_path: str):\n        # コンペの画像は .jpg なのでこうしておく (必要なら .png に変更)\n        img_path = os.path.join(PATH_DATA, image_path)\n        img = Image.open(img_path).convert(\"RGB\")\n        if self.transform is not None:\n            img = self.transform(img)\n        return img\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image = self._load_image(row[\"image_path\"])\n        target = torch.tensor(row[\"target\"], dtype=torch.float32)   # スカラー\n        return image, target\n    \n\nclass BiomassTestDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n    \n    def _load_image(self, image_path):\n        image_path = os.path.join(PATH_DATA, image_path)\n        img = Image.open(image_path).convert(\"RGB\")\n        if self.transform is not None:\n            img = self.transform(img)\n        return img\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image = self._load_image(row[\"image_path\"])\n        sample_id = row[\"sample_id\"]\n        return image, sample_id","metadata":{},"outputs":[],"execution_count":40},{"id":"1b5ad4cd","cell_type":"markdown","source":"簡単に train/valid に分割して DataLoader を作る","metadata":{}},{"id":"44974276","cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, valid_df = train_test_split(\n    df,\n    test_size=0.2,\n    random_state=42,\n    shuffle=True\n)\n\ntrain_dataset = BiomassDataset(train_df, transform=train_transform)\nvalid_dataset = BiomassDataset(valid_df, transform=train_transform)\n\nBATCH_SIZE = 16\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\nvalid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)","metadata":{},"outputs":[],"execution_count":41},{"id":"9e3b9157","cell_type":"markdown","source":"デバイス","metadata":{}},{"id":"5e1a6481","cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\n\ndevice","metadata":{},"outputs":[{"data":{"text/plain":["device(type='mps')"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"execution_count":42},{"id":"de72c7ed","cell_type":"markdown","source":"シンプルな CNN ベースライン(画像 -> グローバル平均プーリング -> 全結合で5ターゲット)を定義","metadata":{}},{"id":"580d5104","cell_type":"code","source":"class SimpleCNNRegressor(nn.Module):\n    def __init__(self, num_targets: int):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),    # 256 -> 128\n\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),    # 128 -> 64\n\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),    # 64 -> 32\n\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),    # 32 -> 16\n        )\n\n        # グローバル平均プーリング\n        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.regressor = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, num_targets)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.global_pool(x)\n        x = self.regressor(x)\n        return x\n    \nEPOCHS = 10\nlr = 1e-3\n\nmodel = SimpleCNNRegressor(num_targets=len(TARGET_COLS)).to(device)\nloss_function = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)","metadata":{},"outputs":[],"execution_count":43},{"id":"8674fd0d","cell_type":"markdown","source":"## 学習ループ(MSE)\n\n最低限の train/valid ループ","metadata":{}},{"id":"154d3779","cell_type":"code","source":"def train_one_epoch(model, loader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    running_abs_error = 0.0\n    n_samples = 0\n\n    for images, targets in tqdm(loader):\n        images = images.to(device)\n        targets = targets.to(device)    # shape: (batch, )\n\n        optimizer.zero_grad()\n        outputs = model(images).squeeze(-1)  # (batch, 1) -> (batch, )\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * images.size(0)\n        running_abs_error += torch.abs(outputs - targets).sum().item()\n        n_samples += images.size(0)\n    \n    epoch_loss = running_loss / n_samples   # MSE\n    epoch_mae = running_abs_error / n_samples   # MAE\n    epoch_rmse = np.sqrt(epoch_loss)   # RMSE\n\n    return epoch_loss, epoch_mae, epoch_rmse\n\n\ndef eval_one_epoch(model, loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    running_abs_error = 0.0\n    n_samples = 0\n\n    with torch.no_grad():\n        for images, targets in tqdm(loader):\n            images = images.to(device)\n            targets = targets.to(device)\n\n            outputs = model(images).squeeze(-1)  # (batch, 1) -> (batch, )\n            loss = criterion(outputs, targets)\n\n            running_loss += loss.item() * images.size(0)\n            running_abs_error += torch.abs(outputs - targets).sum().item()\n            n_samples += images.size(0)\n    \n    epoch_loss = running_loss / n_samples   # MSE\n    epoch_mae = running_abs_error / n_samples  # MAE\n    epoch_rmse = np.sqrt(epoch_loss)   # RMSE\n\n    return epoch_loss, epoch_mae, epoch_rmse","metadata":{},"outputs":[],"execution_count":44},{"id":"f48e7602","cell_type":"code","source":"for epoch in range(EPOCHS):\n    train_loss, train_mae, train_rmse = train_one_epoch(model, train_loader, loss_function, optimizer, device)\n    valid_loss, valid_mae, valid_rmse = eval_one_epoch(model, valid_loader, loss_function, device)\n\n\n    print(\n        f\"Epoch [{epoch}/{EPOCHS}] \"\n        f\"Train [MSE: {train_loss:.4f}, RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}] \"\n        f\"Valid [MSE: {valid_loss:.4f}, RMSE: {valid_rmse:.4f}, MAE: {valid_mae:.4f}]\"\n    )","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 90/90 [00:30<00:00,  2.91it/s]\n","100%|██████████| 23/23 [00:07<00:00,  3.04it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [0/10] Train [MSE: 735.1425, RMSE: 27.1135, MAE: 19.7223] Valid [MSE: 672.0477, RMSE: 25.9239, MAE: 18.5709]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 90/90 [00:31<00:00,  2.88it/s]\n","100%|██████████| 23/23 [00:07<00:00,  2.97it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/10] Train [MSE: 666.0943, RMSE: 25.8088, MAE: 19.0631] Valid [MSE: 646.2884, RMSE: 25.4222, MAE: 19.4794]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 90/90 [00:32<00:00,  2.81it/s]\n","100%|██████████| 23/23 [00:07<00:00,  2.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [2/10] Train [MSE: 677.8992, RMSE: 26.0365, MAE: 19.1968] Valid [MSE: 704.8891, RMSE: 26.5497, MAE: 18.4405]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 90/90 [00:31<00:00,  2.83it/s]\n","100%|██████████| 23/23 [00:07<00:00,  3.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [3/10] Train [MSE: 650.8575, RMSE: 25.5119, MAE: 18.8365] Valid [MSE: 646.3437, RMSE: 25.4233, MAE: 19.3689]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 90/90 [00:31<00:00,  2.85it/s]\n","100%|██████████| 23/23 [00:07<00:00,  3.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [4/10] Train [MSE: 640.1366, RMSE: 25.3009, MAE: 18.8182] Valid [MSE: 639.5758, RMSE: 25.2898, MAE: 19.0140]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 90/90 [00:31<00:00,  2.85it/s]\n","100%|██████████| 23/23 [00:07<00:00,  3.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [5/10] Train [MSE: 642.7628, RMSE: 25.3528, MAE: 18.7579] Valid [MSE: 639.6067, RMSE: 25.2904, MAE: 18.5804]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 90/90 [00:31<00:00,  2.85it/s]\n","100%|██████████| 23/23 [00:07<00:00,  3.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [6/10] Train [MSE: 629.6714, RMSE: 25.0933, MAE: 18.5429] Valid [MSE: 679.3981, RMSE: 26.0653, MAE: 20.4485]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 90/90 [00:31<00:00,  2.85it/s]\n","100%|██████████| 23/23 [00:07<00:00,  3.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [7/10] Train [MSE: 657.6143, RMSE: 25.6440, MAE: 18.7769] Valid [MSE: 646.0814, RMSE: 25.4181, MAE: 18.2245]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 90/90 [00:31<00:00,  2.86it/s]\n","100%|██████████| 23/23 [00:07<00:00,  3.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [8/10] Train [MSE: 622.8731, RMSE: 24.9574, MAE: 18.3455] Valid [MSE: 653.1171, RMSE: 25.5562, MAE: 19.6360]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 90/90 [00:31<00:00,  2.85it/s]\n","100%|██████████| 23/23 [00:07<00:00,  3.01it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch [9/10] Train [MSE: 640.4808, RMSE: 25.3077, MAE: 18.5625] Valid [MSE: 723.3892, RMSE: 26.8959, MAE: 18.4755]\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"execution_count":45},{"id":"13ec49f4","cell_type":"markdown","source":"## テスト実行","metadata":{}},{"id":"dea90a8e","cell_type":"code","source":"PATH_TEST_CSV = os.path.join(PATH_DATA, \"test.csv\")\ntest_df = pd.read_csv(PATH_TEST_CSV)\ntest_df.head()\n\ntest_dataset = BiomassTestDataset(test_df, transform=train_transform)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n\nmodel.eval()\nall_sample_ids = []\nall_preds = []\n\nwith torch.no_grad():\n    for images, sample_ids in tqdm(test_loader):\n        images = images.to(device)\n\n        outputs = model(images).squeeze(-1)\n        preds = outputs.cpu().numpy()\n\n        all_sample_ids.extend(sample_ids)\n        all_preds.extend(preds)\n\nsubmission = pd.DataFrame({\n    \"sample_id\": all_sample_ids,\n    \"target\": all_preds,\n})\n\nprint(submission.head())\nprint(submission.shape)\n\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [00:00<00:00,  8.64it/s]"]},{"name":"stdout","output_type":"stream","text":["                    sample_id     target\n","0  ID1001187975__Dry_Clover_g  13.408747\n","1    ID1001187975__Dry_Dead_g  13.408747\n","2   ID1001187975__Dry_Green_g  13.408747\n","3   ID1001187975__Dry_Total_g  13.408747\n","4         ID1001187975__GDM_g  13.408747\n","(5, 2)\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"execution_count":46}]}